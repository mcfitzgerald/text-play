{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "toxic-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enabling-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': {'train': 'corpus/en-core-web/train.spacy',\n",
       "  'dev': 'corpus/en-core-web/dev.spacy',\n",
       "  'vectors': None,\n",
       "  'raw': None,\n",
       "  'init_tok2vec': None,\n",
       "  'vocab_data': None},\n",
       " 'system': {'gpu_allocator': None, 'seed': 0},\n",
       " 'nlp': {'lang': 'en',\n",
       "  'pipeline': ['tok2vec',\n",
       "   'tagger',\n",
       "   'parser',\n",
       "   'senter',\n",
       "   'ner',\n",
       "   'attribute_ruler',\n",
       "   'lemmatizer'],\n",
       "  'disabled': ['senter'],\n",
       "  'before_creation': None,\n",
       "  'after_creation': None,\n",
       "  'after_pipeline_creation': None,\n",
       "  'batch_size': 256,\n",
       "  'tokenizer': {'@tokenizers': 'spacy.Tokenizer.v1'}},\n",
       " 'components': {'tok2vec': {'factory': 'tok2vec',\n",
       "   'model': {'@architectures': 'spacy.Tok2Vec.v1',\n",
       "    'embed': {'@architectures': 'spacy.MultiHashEmbed.v1',\n",
       "     'width': '${components.tok2vec.model.encode:width}',\n",
       "     'attrs': ['NORM', 'PREFIX', 'SUFFIX', 'SHAPE'],\n",
       "     'rows': [5000, 2500, 2500, 2500],\n",
       "     'include_static_vectors': False},\n",
       "    'encode': {'@architectures': 'spacy.MaxoutWindowEncoder.v1',\n",
       "     'width': 96,\n",
       "     'depth': 4,\n",
       "     'window_size': 1,\n",
       "     'maxout_pieces': 3}}},\n",
       "  'tagger': {'factory': 'tagger',\n",
       "   'model': {'@architectures': 'spacy.Tagger.v1',\n",
       "    'nO': None,\n",
       "    'tok2vec': {'@architectures': 'spacy.Tok2VecListener.v1',\n",
       "     'width': '${components.tok2vec.model.encode:width}',\n",
       "     'upstream': '*'}}},\n",
       "  'parser': {'factory': 'parser',\n",
       "   'learn_tokens': False,\n",
       "   'min_action_freq': 30,\n",
       "   'model': {'@architectures': 'spacy.TransitionBasedParser.v2',\n",
       "    'state_type': 'parser',\n",
       "    'extra_state_tokens': False,\n",
       "    'hidden_width': 64,\n",
       "    'maxout_pieces': 2,\n",
       "    'use_upper': True,\n",
       "    'nO': None,\n",
       "    'tok2vec': {'@architectures': 'spacy.Tok2VecListener.v1',\n",
       "     'width': '${components.tok2vec.model.encode:width}',\n",
       "     'upstream': '*'}},\n",
       "   'moves': None,\n",
       "   'update_with_oracle_cut_size': 100},\n",
       "  'senter': {'factory': 'senter',\n",
       "   'model': {'@architectures': 'spacy.Tagger.v1',\n",
       "    'nO': None,\n",
       "    'tok2vec': {'@architectures': 'spacy.Tok2Vec.v1',\n",
       "     'embed': {'@architectures': 'spacy.MultiHashEmbed.v1',\n",
       "      'width': 16,\n",
       "      'attrs': ['NORM', 'PREFIX', 'SUFFIX', 'SHAPE'],\n",
       "      'rows': [1000, 500, 500, 500],\n",
       "      'include_static_vectors': False},\n",
       "     'encode': {'@architectures': 'spacy.MaxoutWindowEncoder.v1',\n",
       "      'width': 16,\n",
       "      'depth': 2,\n",
       "      'window_size': 1,\n",
       "      'maxout_pieces': 2}}}},\n",
       "  'ner': {'factory': 'ner',\n",
       "   'model': {'@architectures': 'spacy.TransitionBasedParser.v2',\n",
       "    'state_type': 'ner',\n",
       "    'extra_state_tokens': False,\n",
       "    'hidden_width': 64,\n",
       "    'maxout_pieces': 2,\n",
       "    'use_upper': True,\n",
       "    'nO': None,\n",
       "    'tok2vec': {'@architectures': 'spacy.Tok2Vec.v1',\n",
       "     'embed': {'@architectures': 'spacy.MultiHashEmbed.v1',\n",
       "      'width': 96,\n",
       "      'attrs': ['NORM', 'PREFIX', 'SUFFIX', 'SHAPE'],\n",
       "      'rows': [5000, 2500, 2500, 2500],\n",
       "      'include_static_vectors': False},\n",
       "     'encode': {'@architectures': 'spacy.MaxoutWindowEncoder.v1',\n",
       "      'width': 96,\n",
       "      'depth': 4,\n",
       "      'window_size': 1,\n",
       "      'maxout_pieces': 3}}},\n",
       "   'moves': None,\n",
       "   'update_with_oracle_cut_size': 100},\n",
       "  'attribute_ruler': {'factory': 'attribute_ruler', 'validate': False},\n",
       "  'lemmatizer': {'factory': 'lemmatizer',\n",
       "   'mode': 'rule',\n",
       "   'model': None,\n",
       "   'overwrite': False}},\n",
       " 'corpora': {'dev': {'@readers': 'spacy.Corpus.v1',\n",
       "   'limit': 0,\n",
       "   'max_length': 0,\n",
       "   'path': '${paths:dev}',\n",
       "   'gold_preproc': False,\n",
       "   'augmenter': None},\n",
       "  'train': {'@readers': 'spacy.Corpus.v1',\n",
       "   'path': '${paths:train}',\n",
       "   'max_length': 5000,\n",
       "   'gold_preproc': False,\n",
       "   'limit': 0,\n",
       "   'augmenter': {'@augmenters': 'spacy.orth_variants.v1',\n",
       "    'level': 0.2,\n",
       "    'lower': 0.5,\n",
       "    'orth_variants': {'@readers': 'srsly.read_json.v1',\n",
       "     'path': 'assets/orth_variants.json'}}}},\n",
       " 'training': {'train_corpus': 'corpora.train',\n",
       "  'dev_corpus': 'corpora.dev',\n",
       "  'seed': '${system:seed}',\n",
       "  'gpu_allocator': '${system:gpu_allocator}',\n",
       "  'dropout': 0.1,\n",
       "  'accumulate_gradient': 1,\n",
       "  'patience': 5000,\n",
       "  'max_epochs': 0,\n",
       "  'max_steps': 0,\n",
       "  'eval_frequency': 1000,\n",
       "  'frozen_components': [],\n",
       "  'before_to_disk': None,\n",
       "  'batcher': {'@batchers': 'spacy.batch_by_words.v1',\n",
       "   'discard_oversize': False,\n",
       "   'tolerance': 0.2,\n",
       "   'get_length': None,\n",
       "   'size': {'@schedules': 'compounding.v1',\n",
       "    'start': 100,\n",
       "    'stop': 1000,\n",
       "    'compound': 1.001,\n",
       "    't': 0.0}},\n",
       "  'logger': {'@loggers': 'spacy.WandbLogger.v1',\n",
       "   'project_name': 'spacy-v3.0.0a2',\n",
       "   'remove_config_values': []},\n",
       "  'optimizer': {'@optimizers': 'Adam.v1',\n",
       "   'beta1': 0.9,\n",
       "   'beta2': 0.999,\n",
       "   'L2_is_weight_decay': True,\n",
       "   'L2': 0.01,\n",
       "   'grad_clip': 1.0,\n",
       "   'use_averages': True,\n",
       "   'eps': 1e-08,\n",
       "   'learn_rate': 0.001},\n",
       "  'score_weights': {'dep_las_per_type': None,\n",
       "   'sents_p': None,\n",
       "   'sents_r': None,\n",
       "   'ents_per_type': None,\n",
       "   'tag_acc': 0.2,\n",
       "   'dep_uas': 0.0,\n",
       "   'dep_las': 0.0,\n",
       "   'sents_f': 0.4,\n",
       "   'ents_f': 0.2,\n",
       "   'ents_p': 0.0,\n",
       "   'ents_r': 0.0,\n",
       "   'lemma_acc': 0.2}},\n",
       " 'pretraining': {},\n",
       " 'initialize': {'vocab_data': '${paths.vocab_data}',\n",
       "  'vectors': '${paths.vectors}',\n",
       "  'init_tok2vec': '${paths.init_tok2vec}',\n",
       "  'before_init': None,\n",
       "  'after_init': None,\n",
       "  'components': {'ner': {'labels': {'@readers': 'spacy.read_labels.v1',\n",
       "     'path': 'corpus/labels/ner.json',\n",
       "     'require': False}},\n",
       "   'parser': {'labels': {'@readers': 'spacy.read_labels.v1',\n",
       "     'path': 'corpus/labels/parser.json',\n",
       "     'require': False}},\n",
       "   'tagger': {'labels': {'@readers': 'spacy.read_labels.v1',\n",
       "     'path': 'corpus/labels/tagger.json',\n",
       "     'require': False}}},\n",
       "  'lookups': {'@misc': 'spacy.LookupsDataLoader.v1',\n",
       "   'lang': '${nlp.lang}',\n",
       "   'tables': ['lexeme_norm']},\n",
       "  'tokenizer': {}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separated-stewart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x10f7a3a40>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x10f720360>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x10f188d00>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x10f188ac0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x10f26dc40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x10f29ab80>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "superb-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "destroyed-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Net'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'income'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'was'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '$'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '9.4'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'million'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'compared'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'to'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'prior'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'year'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '$'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '2.7'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'million'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'Revenue'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'exceeded'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'twelve'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'billion'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'dollars'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'with'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'a'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'loss'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '$'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '1b'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
      "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-modern",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
